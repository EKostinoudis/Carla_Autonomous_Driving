# use the new RLLib api with RLModule
use_rl_module: true

# if not using RLModule: model name: CIL_multiview_rllib or CIL_multiview_rllib_stack
model: CIL_multiview_rllib

# use CILv2_vec_env instead of CILv2_env
use_vec_env: true
no_rollout_workers: true # if use_vec_env is true, set num_rollout_workers=0

# file to a checkpoint from which we start the training
checkpoint_file: models/CILv2_multiview/_results/checkpoints/CIL_multiview_actor_critic/CIL_multiview_actor_critic_final.pth

# path the the models config file
path_to_conf: ./models/CILv2_multiview/_results/Ours/Town12346_5/config40.json

# for the value function (critic) pretraining
pretrain_value: true # enables pretraining
pretrain_iters: 2
pretrain_complete_episodes: true # use batch_mode: complete_episodes for the pretraining
lr_pretrain: 5.0e-05 # lr for pretraining (null or remove it to use the same as training)

train_iters: 1000

# for sampling
num_workers: 8 # number of parallel environments
num_cpus_per_worker: 1
worker_gpus: 0 # number of gpus for the sampling (collecting the observations)

training_gpus: 4 # number of gpus for the training (updating the weights)

# for training
num_learner_workers: 0
num_cpus_per_learner_worker: 1
num_gpus_per_learner_worker: 0

checkpoint_freq: 20 # frequency for the checkpoints

# extra parameters for the PPOConfig
extra_params:
  disable_env_checking: true
  explore: false # default true

  lr: 5.0e-06
  train_batch_size: 2000
  sgd_minibatch_size: 128
  num_sgd_iter: 30

  gamma: 0.99
  clip_param: 0.2

  vf_clip_param: 10.0
  vf_loss_coeff: 1.0

  use_gae: true
  lambda_: 0.95

  use_kl_loss: true
  kl_coeff: 0.2
  kl_target: 0.01

  shuffle_sequences: true
  entropy_coeff: 0.01
  entropy_coeff_schedule: null

  # _disable_preprocessor_api: true # in older version uses numpy array if used???
  recreate_failed_workers: false # default false
  restart_failed_sub_environments: false # default false

  torch_compile_learner: true # default false
  torch_compile_learner_dynamo_backend: ipex # inductor
  torch_compile_learner_dynamo_mode: max-autotune

  torch_compile_worker: true # default false
  torch_compile_worker_dynamo_backend: inductor

  worker_restore_timeout_s: 1800.0 # 180.0
  worker_health_probe_timeout_s: 600 # 60
  sync_filters_on_rollout_workers_timeout_s: 600.0 # 60.0
  metrics_episode_collection_timeout_s: 600.0 60.0
  evaluation_sample_timeout_s: 1800.0 # 180.0

  # causes an error, do NOT set to true
  # _disable_initialize_loss_from_dummy_batch: false # default false

  # explore: true
  # exploration_config:
  #   type: StochasticSampling
